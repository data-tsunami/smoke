# -*- coding: utf-8 -*-
#@PydevCodeAnalysisIgnore

from __future__ import unicode_literals

LINES = (
"""Spark assembly has been built with Hive, including Datanucleus jars on classpath""",
"""14/09/13 12:28:29 INFO spark.SecurityManager: Changing view acls to: hadoop""",
"""14/09/13 12:28:29 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop)""",
"""14/09/13 12:28:29 INFO spark.HttpServer: Starting HTTP Server""",
"""14/09/13 12:28:29 INFO server.Server: jetty-8.y.z-SNAPSHOT""",
"""14/09/13 12:28:29 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:59100
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.0.2
      /_/
Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.6.0_31)
Type in expressions to have them evaluated.
Type :help for more information.""",
"""14/09/13 12:28:32 INFO spark.SecurityManager: Changing view acls to: hadoop""",
"""14/09/13 12:28:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop)""",
"""14/09/13 12:28:33 INFO slf4j.Slf4jLogger: Slf4jLogger started""",
"""14/09/13 12:28:33 INFO Remoting: Starting remoting""",
"""14/09/13 12:28:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@cluster-master.hadoop.dev.docker.data-tsunami.com:56640]""",
"""14/09/13 12:28:33 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@cluster-master.hadoop.dev.docker.data-tsunami.com:56640]""",
"""14/09/13 12:28:33 INFO spark.SparkEnv: Registering MapOutputTracker""",
"""14/09/13 12:28:33 INFO spark.SparkEnv: Registering BlockManagerMaster""",
"""14/09/13 12:28:33 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20140913122833-f2e3""",
"""14/09/13 12:28:33 INFO storage.MemoryStore: MemoryStore started with capacity 294.4 MB.""",
"""14/09/13 12:28:33 INFO network.ConnectionManager: Bound socket to port 40176 with id = ConnectionManagerId(cluster-master.hadoop.dev.docker.data-tsunami.com,40176)""",
"""14/09/13 12:28:33 INFO storage.BlockManagerMaster: Trying to register BlockManager""",
"""14/09/13 12:28:33 INFO storage.BlockManagerInfo: Registering block manager cluster-master.hadoop.dev.docker.data-tsunami.com:40176 with 294.4 MB RAM""",
"""14/09/13 12:28:33 INFO storage.BlockManagerMaster: Registered BlockManager""",
"""14/09/13 12:28:33 INFO spark.HttpServer: Starting HTTP Server""",
"""14/09/13 12:28:33 INFO server.Server: jetty-8.y.z-SNAPSHOT""",
"""14/09/13 12:28:33 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:51099""",
"""14/09/13 12:28:33 INFO broadcast.HttpBroadcast: Broadcast server started at http://172.17.0.4:51099""",
"""14/09/13 12:28:33 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-2abf9a27-13eb-493a-8674-dbf1c3ff0a78""",
"""14/09/13 12:28:33 INFO spark.HttpServer: Starting HTTP Server""",
"""14/09/13 12:28:33 INFO server.Server: jetty-8.y.z-SNAPSHOT""",
"""14/09/13 12:28:33 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:55699""",
"""14/09/13 12:28:33 INFO server.Server: jetty-8.y.z-SNAPSHOT""",
"""14/09/13 12:28:33 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040""",
"""14/09/13 12:28:33 INFO ui.SparkUI: Started SparkUI at http://cluster-master.hadoop.dev.docker.data-tsunami.com:4040""",
"""14/09/13 12:28:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
--args is deprecated. Use --arg instead.""",
"""14/09/13 12:28:34 INFO client.RMProxy: Connecting to ResourceManager at cluster-master.hadoop.dev.docker.data-tsunami.com/172.17.0.4:8032""",
"""14/09/13 12:28:34 INFO yarn.Client: Got Cluster metric info from ApplicationsManager (ASM), number of NodeManagers: 3""",
"""14/09/13 12:28:34 INFO yarn.Client: Queue info ... queueName: default, queueCurrentCapacity: 0.0, queueMaxCapacity: 1.0,
      queueApplicationCount = 0, queueChildQueueCount = 0""",
"""14/09/13 12:28:34 INFO yarn.Client: Max mem capabililty of a single resource in this cluster 8192""",
"""14/09/13 12:28:34 INFO yarn.Client: Preparing Local resources""",
"""14/09/13 12:28:35 INFO yarn.Client: Uploading file:/home/hadoop/spark-1.0.2-bin-hadoop2/lib/spark-assembly-1.0.2-hadoop2.2.0.jar to hdfs://cluster-master.hadoop.dev.docker.data-tsunami.com:9000/user/hadoop/.sparkStaging/application_1410620475517_0004/spark-assembly-1.0.2-hadoop2.2.0.jar""",
"""14/09/13 12:28:36 INFO yarn.Client: Setting up the launch environment""",
"""14/09/13 12:28:36 INFO yarn.Client: Setting up container launch context""",
"""14/09/13 12:28:36 INFO yarn.Client: Command for starting the Spark ApplicationMaster: List($JAVA_HOME/bin/java, -server, -Xmx512m, -Djava.io.tmpdir=$PWD/tmp, -Dspark.tachyonStore.folderName=\"spark-017e9add-fbae-41ab-b78b-11a4073c8a42\", -Dspark.yarn.secondary.jars=\"\", -Dspark.home=\"/home/hadoop/spark-1.0.2-bin-hadoop2\", -Dspark.repl.class.uri=\"http://172.17.0.4:59100\", -Dspark.driver.host=\"cluster-master.hadoop.dev.docker.data-tsunami.com\", -Dspark.app.name=\"Spark shell\", -Dspark.jars=\"\", -Dspark.fileserver.uri=\"http://172.17.0.4:55699\", -Dspark.master=\"yarn-client\", -Dspark.driver.port=\"56640\", -Dspark.executor.cores=\"2\", -Dspark.httpBroadcast.uri=\"http://172.17.0.4:51099\",  -Dlog4j.configuration=log4j-spark-container.properties, org.apache.spark.deploy.yarn.ExecutorLauncher, --class, notused, --jar , null,  --args  'cluster-master.hadoop.dev.docker.data-tsunami.com:56640' , --executor-memory, 1024, --executor-cores, 2, --num-executors , 2, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)""",
"""14/09/13 12:28:36 INFO yarn.Client: Submitting application to ASM""",
"""14/09/13 12:28:36 INFO impl.YarnClientImpl: Submitted application application_1410620475517_0004 to ResourceManager at cluster-master.hadoop.dev.docker.data-tsunami.com/172.17.0.4:8032""",
"""14/09/13 12:28:36 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
   appMasterRpcPort: -1
   appStartTime: 1410622116205
   yarnAppState: ACCEPTED""",
"""14/09/13 12:28:37 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
   appMasterRpcPort: -1
   appStartTime: 1410622116205
   yarnAppState: ACCEPTED""",
"""14/09/13 12:28:38 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
   appMasterRpcPort: -1
   appStartTime: 1410622116205
   yarnAppState: ACCEPTED""",
"""14/09/13 12:28:39 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
   appMasterRpcPort: -1
   appStartTime: 1410622116205
   yarnAppState: ACCEPTED""",
"""14/09/13 12:28:40 INFO cluster.YarnClientSchedulerBackend: Application report from ASM:
   appMasterRpcPort: 0
   appStartTime: 1410622116205
   yarnAppState: RUNNING""",
"""14/09/13 12:28:41 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com:43768/user/Executor#1024535204] with ID 1""",
"""14/09/13 12:28:42 INFO storage.BlockManagerInfo: Registering block manager hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com:55437 with 588.8 MB RAM""",
"""14/09/13 12:28:42 INFO cluster.YarnClientClusterScheduler: YarnClientClusterScheduler.postStartHook done""",
"""14/09/13 12:28:42 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.
Loading /tmp/spark-job-script-t753f0lH5Y.scala...""",
"""14/09/13 12:28:42 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com:53571/user/Executor#-459671133] with ID 2""",
"""14/09/13 12:28:43 INFO storage.BlockManagerInfo: Registering block manager hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com:57529 with 588.8 MB RAM
defined module DataTsunamiUtils""",
"""14/09/13 12:28:46 INFO storage.MemoryStore: ensureFreeSpace(73020) called with curMem=0, maxMem=308713881""",
"""14/09/13 12:28:46 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 71.3 KB, free 294.3 MB)""",
"""14/09/13 12:28:47 INFO mapred.FileInputFormat: Total input paths to process : 1""",
"""14/09/13 12:28:47 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id""",
"""14/09/13 12:28:47 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id""",
"""14/09/13 12:28:47 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap""",
"""14/09/13 12:28:47 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition""",
"""14/09/13 12:28:47 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id""",
"""14/09/13 12:28:47 INFO spark.SparkContext: Starting job: saveAsTextFile at <console>:93""",
"""14/09/13 12:28:47 INFO scheduler.DAGScheduler: Registering RDD 3 (map at <console>:74)""",
"""14/09/13 12:28:47 INFO scheduler.DAGScheduler: Got job 0 (saveAsTextFile at <console>:93) with 10 output partitions (allowLocal=false)""",
"""14/09/13 12:28:47 INFO scheduler.DAGScheduler: Final stage: Stage 0(saveAsTextFile at <console>:93)""",
"""14/09/13 12:28:47 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 1)""",
"""14/09/13 12:28:47 INFO scheduler.DAGScheduler: Missing parents: List(Stage 1)""",
"""14/09/13 12:28:47 INFO scheduler.DAGScheduler: Submitting Stage 1 (MappedRDD[3] at map at <console>:74), which has no missing parents""",
"""14/09/13 12:28:47 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from Stage 1 (MappedRDD[3] at map at <console>:74)""",
"""14/09/13 12:28:47 INFO cluster.YarnClientClusterScheduler: Adding task set 1.0 with 10 tasks""",
"""14/09/13 12:28:47 INFO util.RackResolver: Resolved hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com to /default-rack""",
"""14/09/13 12:28:47 INFO util.RackResolver: Resolved hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com to /default-rack""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Starting task 1.0:0 as TID 0 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as 2357 bytes in 4 ms""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Starting task 1.0:1 as TID 1 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Serialized task 1.0:1 as 2357 bytes in 0 ms""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Starting task 1.0:2 as TID 2 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Serialized task 1.0:2 as 2357 bytes in 0 ms""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Starting task 1.0:4 as TID 3 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:47 INFO scheduler.TaskSetManager: Serialized task 1.0:4 as 2357 bytes in 1 ms""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Starting task 1.0:5 as TID 4 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Serialized task 1.0:5 as 2357 bytes in 0 ms""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Starting task 1.0:6 as TID 5 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Serialized task 1.0:6 as 2357 bytes in 0 ms""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Finished TID 3 in 5775 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 1/10)""",
"""14/09/13 12:28:53 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 4)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Finished TID 1 in 5782 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 2/10)""",
"""14/09/13 12:28:53 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 1)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Starting task 1.0:3 as TID 6 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Serialized task 1.0:3 as 2357 bytes in 1 ms""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Starting task 1.0:8 as TID 7 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Serialized task 1.0:8 as 2357 bytes in 1 ms""",
"""14/09/13 12:28:53 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 2)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Finished TID 2 in 5812 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 3/10)""",
"""14/09/13 12:28:53 INFO scheduler.TaskSetManager: Finished TID 0 in 5836 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 4/10)""",
"""14/09/13 12:28:53 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 0)""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Starting task 1.0:7 as TID 8 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (RACK_LOCAL)""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Serialized task 1.0:7 as 2357 bytes in 1 ms""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Finished TID 7 in 3978 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 5/10)""",
"""14/09/13 12:28:57 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 8)""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Starting task 1.0:9 as TID 9 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (NODE_LOCAL)""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Serialized task 1.0:9 as 2357 bytes in 0 ms""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Finished TID 5 in 4101 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 6/10)""",
"""14/09/13 12:28:57 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 6)""",
"""14/09/13 12:28:57 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 5)""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Finished TID 4 in 4248 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 7/10)""",
"""14/09/13 12:28:57 INFO scheduler.TaskSetManager: Finished TID 6 in 4229 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 8/10)""",
"""14/09/13 12:28:57 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 3)""",
"""14/09/13 12:28:59 INFO scheduler.TaskSetManager: Finished TID 9 in 2600 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 9/10)""",
"""14/09/13 12:28:59 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 9)""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Finished TID 8 in 3409 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 10/10)""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 7)""",
"""14/09/13 12:29:00 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: Stage 1 (map at <console>:74) finished in 13.215 s""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: looking for newly runnable stages""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: running: Set()""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: waiting: Set(Stage 0)""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: failed: Set()""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: Missing parents for Stage 0: List()""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[8] at saveAsTextFile at <console>:93), which is now runnable""",
"""14/09/13 12:29:00 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[8] at saveAsTextFile at <console>:93)""",
"""14/09/13 12:29:00 INFO cluster.YarnClientClusterScheduler: Adding task set 0.0 with 10 tasks""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID 10 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as 12073 bytes in 1 ms""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Starting task 0.0:1 as TID 11 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Serialized task 0.0:1 as 12073 bytes in 0 ms""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Starting task 0.0:2 as TID 12 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Serialized task 0.0:2 as 12073 bytes in 1 ms""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Starting task 0.0:3 as TID 13 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:00 INFO scheduler.TaskSetManager: Serialized task 0.0:3 as 12073 bytes in 1 ms""",
"""14/09/13 12:29:00 INFO spark.MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 0 to spark@hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com:49468""",
"""14/09/13 12:29:00 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 250 bytes""",
"""14/09/13 12:29:00 INFO spark.MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 0 to spark@hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com:40878""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Starting task 0.0:4 as TID 14 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Serialized task 0.0:4 as 12073 bytes in 0 ms""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Finished TID 10 in 1673 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 1/10)""",
"""14/09/13 12:29:02 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Starting task 0.0:5 as TID 15 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Serialized task 0.0:5 as 12073 bytes in 0 ms""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Finished TID 11 in 1676 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 2/10)""",
"""14/09/13 12:29:02 INFO scheduler.DAGScheduler: Completed ResultTask(0, 1)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Starting task 0.0:6 as TID 16 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Serialized task 0.0:6 as 12073 bytes in 0 ms""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Finished TID 13 in 1693 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 3/10)""",
"""14/09/13 12:29:02 INFO scheduler.DAGScheduler: Completed ResultTask(0, 3)""",
"""14/09/13 12:29:02 INFO scheduler.DAGScheduler: Completed ResultTask(0, 2)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Finished TID 12 in 1703 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 4/10)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Starting task 0.0:7 as TID 17 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Serialized task 0.0:7 as 12073 bytes in 0 ms""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Starting task 0.0:8 as TID 18 on executor 1: hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Serialized task 0.0:8 as 12073 bytes in 0 ms""",
"""14/09/13 12:29:02 INFO scheduler.DAGScheduler: Completed ResultTask(0, 7)""",
"""14/09/13 12:29:02 INFO scheduler.TaskSetManager: Finished TID 17 in 627 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 5/10)""",
"""14/09/13 12:29:03 INFO scheduler.TaskSetManager: Starting task 0.0:9 as TID 19 on executor 2: hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (PROCESS_LOCAL)""",
"""14/09/13 12:29:03 INFO scheduler.TaskSetManager: Serialized task 0.0:9 as 12073 bytes in 0 ms""",
"""14/09/13 12:29:03 INFO scheduler.TaskSetManager: Finished TID 16 in 643 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 6/10)""",
"""14/09/13 12:29:03 INFO scheduler.DAGScheduler: Completed ResultTask(0, 6)""",
"""14/09/13 12:29:03 INFO scheduler.TaskSetManager: Finished TID 15 in 707 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 7/10)""",
"""14/09/13 12:29:03 INFO scheduler.DAGScheduler: Completed ResultTask(0, 5)""",
"""14/09/13 12:29:03 INFO scheduler.DAGScheduler: Completed ResultTask(0, 4)""",
"""14/09/13 12:29:03 INFO scheduler.TaskSetManager: Finished TID 14 in 720 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 8/10)""",
"""14/09/13 12:29:03 INFO scheduler.TaskSetManager: Finished TID 18 in 409 ms on hadoop-wd250gb.hadoop.dev.docker.data-tsunami.com (progress: 9/10)""",
"""14/09/13 12:29:03 INFO scheduler.DAGScheduler: Completed ResultTask(0, 8)""",
"""14/09/13 12:29:03 INFO scheduler.DAGScheduler: Completed ResultTask(0, 9)""",
"""14/09/13 12:29:03 INFO scheduler.TaskSetManager: Finished TID 19 in 410 ms on hadoop2-seagate2tb.hadoop.dev.docker.data-tsunami.com (progress: 10/10)""",
"""14/09/13 12:29:03 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool""",
"""14/09/13 12:29:03 INFO scheduler.DAGScheduler: Stage 0 (saveAsTextFile at <console>:93) finished in 2.750 s""",
"""14/09/13 12:29:03 INFO spark.SparkContext: Job finished: saveAsTextFile at <console>:93, took 16.188852357 s""",
"""@@<msgFromShell cookie="00b623f78474403490e352cd02e1c423"><outputFileName>/movies/output-1410622127179</outputFileName></msgFromShell>@@""",
"""warning: there were 1 deprecation warning(s); re-run with -deprecation for details""",
)
